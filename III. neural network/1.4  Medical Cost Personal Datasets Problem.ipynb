{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Its time to test Ourself with a more real life example with Large DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the insurance dataset\n",
    "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n",
    "insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try one hot encode our dataframe\n",
    "insurance_one_hot = pd.get_dummies(insurance)\n",
    "insurance_one_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X & Y values (features and labels)\n",
    "X = insurance_one_hot.drop(\"charges\", axis=1)\n",
    "y = insurance_one_hot[\"charges\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View y\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Explinance\n",
    "# test_size = 0.2, here is were we do the splitting 80-20 ratio of the data. 0.2 or 20% of the data is dedicated to testing\n",
    "# random_state =42 makes sure that the data is split evenly each time\n",
    "\n",
    "\n",
    "# Lets see the data\n",
    "len(X), len(X_train), len(X_test)\n",
    "\n",
    "# Explinance\n",
    "# len(X) is all the data we have\n",
    "# len(X_train) is all the data we use for training\n",
    "# len(X_test) is all the data we use for testing, in this case 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Neural Network\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "inscurance_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10),\n",
    "        tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "inscurance_model.compile(loss = tf.keras.layers.mae,\n",
    "                optimizer = tf.keras.optimizers.SGD(),\n",
    "                metrics = [\"mae\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "inscurance_model.fit(X_train, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the insurance model on the test data\n",
    "inscurance_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now it loook like our model isnt performing too well, so lets start to improve it.\n",
    "\n",
    "To (try) to improve the model we could do:\n",
    "1. Add an extra layer with more hidden units and use the Adam optimizer\n",
    "2. Train for longer (200 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Hidden units Experiment\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "inscurance_model_2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(100),\n",
    "        tf.keras.layers.Dense(10),\n",
    "        tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "inscurance_model_2.compile(loss = tf.keras.layers.mae,\n",
    "                optimizer = tf.keras.optimizers.Adam(), # switched to Adam\n",
    "                metrics = [\"mae\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "inscurance_model_2.fit(X_train, y_train, epochs = 100, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the insurance model 2 on the test data\n",
    "inscurance_model_2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Epochs \n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "inscurance_model_3 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(100),\n",
    "        tf.keras.layers.Dense(10),\n",
    "        tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "inscurance_model_3.compile(loss = tf.keras.layers.mae,\n",
    "                optimizer = tf.keras.optimizers.Adam(), # switched to Adam\n",
    "                metrics = [\"mae\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "inscurance_model_3.fit(X_train, y_train, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the insurance model 3 on the test data\n",
    "history = inscurance_model_3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Loss of the training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot History\n",
    "Also known as a loss curve or training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data (normalization and standardization)\n",
    "\n",
    "In terms of scaling values, neural networks tend to prefer normalization.\n",
    "\n",
    "If, you are not sure what to use you can perform both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"age\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"bmi\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Read in the insurance dataset\n",
    "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n",
    "insurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data , we could borrow a few classes from the Scikit-Learn Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a column transformer\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # turn all values in these columns between 0 and 1\n",
    "    (OneHotEncoder(handle_uknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"]),\n",
    ")\n",
    "\n",
    "# Create the X and y \n",
    "X = insurance.drop(\"charges\", axis=1)\n",
    "y = insurance[\"charges\"]\n",
    "\n",
    "# Build our train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the column transformer to our training data\n",
    "\n",
    "ct.fit(X_train)\n",
    "\n",
    "# Transform training and test data with normalization (MinMaxScaler) and OneHotEncoder\n",
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does our data look like\n",
    "X_train_normal[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has been normalized and one hot ecoded. Now lets build our neural network model on it and see how it goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "inscurance_model_4 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(100),\n",
    "        tf.keras.layers.Dense(10),\n",
    "        tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "inscurance_model_4.compile(loss = tf.keras.layers.mae,\n",
    "                optimizer = tf.keras.optimizers.Adam(), # switched to Adam\n",
    "                metrics = [\"mae\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "inscurance_model_4.fit(X_train, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the insurance model 4 on the test data\n",
    "history = inscurance_model_4.evaluate(X_test_normal, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
