{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Exercise\n",
    "With all knowledge acquired till this point, try to build our own Neural Network Regression with Tensrflow. Here are the specifications:\n",
    "\n",
    "1. Create your own regression dataset (or make the one we created in \"Create data to view and fit\" bigger) and build fit a model to it.\n",
    "2. Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?\n",
    "3. Try and improve the results we got on the insurance dataset, some things you might want to try include:\n",
    " * Building a larger model (how does one with 4 dense layers go?).\n",
    " * Increasing the number of units in each layer.\n",
    " * Lookup the documentation of Adam and find out what the first parameter is, \n",
    "    what happens if you  increase it by 10x?\n",
    " * What happens if you train for longer (say 300 epochs instead of 200)?\n",
    "\n",
    "- Import the Boston pricing dataset from TensorFlow tf.keras.datasets and model it.\n",
    "\n",
    "Here is some data : https://raw.githubusercontent.com/ywchiu/riii/master/data/house-prices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the house prices dataset\n",
    "prices = pd.read_csv(\"https://raw.githubusercontent.com/ywchiu/riii/master/data/house-prices.csv\")\n",
    "prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data , we could borrow a few classes from the Scikit-Learn Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a column transformer\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), [\"SqFt\", \"Bedrooms\", \"Bathrooms\"]), # turn all values in these columns between 0 and 1\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), [\"Offers\", \"Brick\"]),\n",
    ")\n",
    "\n",
    "# Create the X and y \n",
    "X = prices.drop(\"Price\", axis=1)\n",
    "y = prices[\"Price\"]\n",
    "\n",
    "# Build our train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the column transformer to our training data\n",
    "\n",
    "ct.fit(X_train)\n",
    "\n",
    "# Transform training and test data with normalization (MinMaxScaler) and OneHotEncoder\n",
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does our data look like\n",
    "X_train_normal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "ppredictions_model_1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(100),\n",
    "        tf.keras.layers.Dense(10),\n",
    "        tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "ppredictions_model_1.compile(loss=tf.keras.losses.mae,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"mae\"])\n",
    "# 3. Fit the model\n",
    "ppredictions_model_1.fit(X_train_normal, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the insurance model 4 on the test data\n",
    "history = ppredictions_model_1.evaluate(X_test_normal, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to predict with 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "ppredictions_model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(50),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "ppredictions_model_2.compile(loss=tf.keras.losses.mae,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"mae\"])\n",
    "# 3. Fit the model\n",
    "\n",
    "ppredictions_model_2.fit(X_train_normal, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the insurance model 2 on the test data\n",
    "history2 = ppredictions_model_2.evaluate(X_test_normal, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1, price prediction mae = 127064\n",
    "Model 2, price prediction mae = 17466\n",
    "\n",
    "As noted, model 2 with 4 layers preformed better than model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to make other changes to the model\n",
    "\n",
    "200 epochs, learning rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "ppredictions_model_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(50),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "ppredictions_model_3.compile(loss=tf.keras.losses.mae,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=[\"mae\"])\n",
    "# 3. Fit the model\n",
    "\n",
    "ppredictions_model_3.fit(X_train_normal, y_train, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the insurance model 3 on the test data\n",
    "history3 = ppredictions_model_3.evaluate(X_test_normal, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1, price prediction mae = 127064\n",
    "Model 2, price prediction mae = 17466\n",
    "Model 3, price prediction mae = 10473\n",
    "\n",
    "As noted, model 3 is the best model so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Experiment\n",
    "\n",
    "300 epochs, learning rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "ppredictions_model_4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(50),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "ppredictions_model_4.compile(loss=tf.keras.losses.mae,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=[\"mae\"])\n",
    "# 3. Fit the model\n",
    "\n",
    "ppredictions_model_4.fit(X_train_normal, y_train, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the insurance model 4 on the test data\n",
    "history4 = ppredictions_model_4.evaluate(X_test_normal, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1, price prediction mae = 127064\n",
    "Model 2, price prediction mae = 17466\n",
    "Model 3, price prediction mae = 10473\n",
    "Model 4, price prediction mae = 11836.3428\n",
    "\n",
    "As noted, model 3 is the best model so far. Model 4 is overfitting "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
