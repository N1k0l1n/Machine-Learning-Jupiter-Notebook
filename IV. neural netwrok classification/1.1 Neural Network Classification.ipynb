{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a classification problem?\n",
    "\n",
    "There are many classification types of problems:\n",
    "* Binary classification (one thing or an other)\n",
    "* Multiclass classification (more than one thing or an other)\n",
    "* Multilabel classification (multiple label options per sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture of a Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table ><tr><th >HyperParameter <th><th> Binary Classification <th><th> Multiclass Classification <tr><tr>\n",
    "<tr><td> Input Layer Shape<td><td> Same Number of Features <td><td>Same as bianary classification<td><tr>\n",
    "<tr><td> Hidden Layer<td><td> Problem specific, minimum = 1, maximum = unlimited <td><td>Same as bianary classification<td><tr>\n",
    "<tr><td> Neurons per Hidden Layer<td><td> Problem specific, generally 10 to 100 <td><td>Same as bianary classification<td><tr>\n",
    "<tr><td> Output Layer Shape<td><td> 1 (one class or the other) <td><td>1 per class <td><tr>\n",
    "<tr><td> Hidden Activation<td><td> Usually ReLU (reactified lienar unit) <td><td>Same as bianary classification<td><tr>\n",
    "<tr><td> Output Activation<td><td> Sigmoid <td><td>Softmax<td><tr>\n",
    "<tr><td> Loss Function<td><td> Cross Entropy <td><td>Cross Entropy<td><tr>\n",
    "<tr><td> Optimizer <td><td> SGD; Adam <td><td>Same as bianary classification<td><tr><table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1- `Hyperparameters` are the configurations that are set prior to the training process and cannot be directly learned from the data. They govern the behavior of the learning algorithm and impact the performance of the model.\n",
    "\n",
    "* `The input layer shape` refers to the dimensions or structure of the input data that is fed into a neural network model during the training or prediction process. It represents the number of input features or variables that are used to make predictions. For example: In image classification tasks, the input layer shape might be represented as (width, height, channels), where width and height are the dimensions of the image, and channels represent the color channels (e.g., RGB).\n",
    "\n",
    "* `A hidden layer` in a neural network is a layer between the input layer and the output layer. It is where the neural network learns to represent the features of the input data in a way that is useful for making predictions or classifications.\n",
    "\n",
    "* `The number of neurons per hidden layer` in a neural network is a hyperparameter that needs to be specified by the user. It determines the capacity and complexity of the neural network and can significantly impact its ability to learn and generalize from the training data.\n",
    "\n",
    "* `The output layer shape` in a neural network refers to the dimensions or structure of the output data that the network produces after processing the input data. It depends on the type of task the neural network is designed to perform.\n",
    "\n",
    "* `The hidden activation function`, also known as the activation function for the hidden layers, is a mathematical function applied to the outputs of neurons in the hidden layers of a neural network. It introduces non-linearity to the network, enabling it to learn complex patterns in the data and make better predictions.\n",
    "\n",
    "* `The output activation function`, as the name suggests, is the activation function applied to the output layer of a neural network. It transforms the raw output values produced by the network into a format suitable for the specific task at hand.\n",
    "\n",
    "* `The loss function`, also known as the cost function or objective function, is a crucial component of a machine learning model, particularly in supervised learning tasks. It quantifies the difference between the predicted values generated by the model and the actual ground truth values from the dataset.\n",
    "\n",
    "* `The optimizer` in machine learning is an algorithm used to adjust the parameters (weights and biases) of a neural network during the training process. Its primary goal is to minimize the loss function by iteratively updating the model's parameters based on the gradients of the loss function with respect to those parameters. The choice of optimizer can significantly impact the training speed, convergence, and generalization performance of the neural network. Here are some common optimizers used in deep learning:\n",
    "- `Stochastic Gradient Descent (SGD)`: SGD is the most basic optimization algorithm.\n",
    "It updates the parameters in the direction of the negative gradient of the loss function with respect to each parameter.\n",
    "SGD can be slow and prone to oscillations, especially in high-dimensional parameter spaces.\n",
    "- `Adam (Adaptive Moment Estimation)`: Adam is a popular and widely used optimizer.\n",
    "It combines the ideas of momentum and RMSprop to adaptively adjust the learning rate for each parameter.\n",
    "Adam maintains a separate learning rate for each parameter and keeps track of exponentially decaying average of past gradients and their squares. Adam typically performs well across a wide range of tasks and is less sensitive to hyperparameters compared to SGD.\n",
    "- `RMSprop (Root Mean Square Propagation)`: RMSprop is an optimizer that divides the learning rate for a parameter by a running average of the magnitudes of recent gradients for that parameter. It adapts the learning rate based on the average of the squared gradients, which can improve convergence in deep networks.\n",
    "- `AdaGrad (Adaptive Gradient Algorithm)`: AdaGrad adapts the learning rate for each parameter based on the historical gradient magnitudes. It divides the learning rate by the square root of the sum of the squared gradients for each parameter up to the current time step. AdaGrad performs well for sparse data but may decay the learning rate too aggressively for some problems.\n",
    "- `Adadelta`: Adadelta is an extension of AdaGrad that addresses its tendency to monotonically decrease the learning rate.\n",
    "It maintains a running average of both squared gradients and squared parameter updates, and adapts the learning rate based on the ratio of these two running averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Lecture:\n",
    "[text](https://karpathy.github.io/2019/04/25/recipe/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
