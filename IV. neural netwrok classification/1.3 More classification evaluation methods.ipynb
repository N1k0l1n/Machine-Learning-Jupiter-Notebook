{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More classification evaluation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table ><tr><th >Metric Name <th><th> Metric Formula <th><th> Formula <th><th> When to use  <tr><tr>\n",
    "<tr><td>Accuracy<td><td> Accuracy = tp+ tn/ tp + tn + fp +fn <td><td>tf.keras.metrics.Accuracy()<td><td>Default metric for classification problems. Not the best for imbalanced classes<td><tr>\n",
    "<tr><td>Precision<td><td> Precision = tp/ tp + fp<td><td>tf.keras.metrics.Precision()<td><td>Higher precision leads to false positives<td><tr>\n",
    "<tr><td>Recall<td><td> Recall = tp/ tp + fn<td><td>tf.keras.metrics.Recall()<td><td>Higher recalls leads to false negatives<td><tr>\n",
    "<tr><td>F1-score<td><td> f1_score = 2 * precision * recall/ precision + recall<td><td>tf.keras.metrics.f1_score()<td><td>Combination of precision and recall usually a good overall metric for a classification model<td><tr>\n",
    "<tr><td>Confusion matrix<td><td> NA <td><td>tf.keras.metrics.confusion_matrix()<td><td>When comapring predictions to truth labels to see where model gets confused. Can be hard to use with large number of classes<td><tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
