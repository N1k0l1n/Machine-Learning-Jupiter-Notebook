{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Make 1000 examples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples,\n",
    "                    noise=0.03,\n",
    "                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the features\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the labels\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our details a little hard to understand right now, so let's try to visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "circles = pd.DataFrame({\"X0\":X[:,0], \"X1\":X[:, 1], \"label\": y})\n",
    "circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with a plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Rd.YlBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Output Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of our features and labels\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples we are working with\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first example of features and labels\n",
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Set the random seed\n",
    "tf.ramdom.seed(42)\n",
    "\n",
    "# 1.Create the model usingthe Sequential API\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.Dense(1)\n",
    "])\n",
    "\n",
    "#2. Compile the model\n",
    "model_1.compile(loss= tf.keras.losses.BinaryCrossentropy,\n",
    "                optimizer = tf.train.keras.SGD(),\n",
    "                metrics = \"accuracy\")\n",
    "\n",
    "# 3. FIt the model\n",
    "model_1.fit(X, y, epochs =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporve the model with more training\n",
    "model_1.fit(X, y, epochs =200, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another layer\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.Dense(10),\n",
    "    tf.keras.Dense(1)\n",
    "])\n",
    "\n",
    "model_1.fit(X, y, epochs =100, verbose = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "tf.ramdom.seed(42)\n",
    "\n",
    "# 1.Create the model usingthe Sequential API\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.Dense(100),\n",
    "    tf.keras.Dense(10),\n",
    "    tf.keras.Dense(1)\n",
    "])\n",
    "\n",
    "#2. Compile the model\n",
    "model_2.compile(loss= tf.keras.losses.BinaryCrossentropy,\n",
    "                optimizer = tf.train.keras.Adam(),\n",
    "                metrics = \"accuracy\")\n",
    "\n",
    "# 3. FIt the model\n",
    "model_2.fit(X, y, epochs =100, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model \n",
    "model_2.evaluate(X, y)\n",
    "\n",
    "# Still shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To visualize our model predictions we create a function\n",
    "* Take in a trained model, feature (X) and label (y)\n",
    "* Create a meshgrid of the different X values\n",
    "* Make predictions across the meshgrid\n",
    "* Plot the predictions as well as a line between zones (where each unique class falls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize our model predictions we create a function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"Plots the decision boundary created by the model predictions on X\"\"\"\n",
    "\n",
    "    # Define the axis boundaries of the plot and create a meshgrid\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    # Create X value\n",
    "    x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_in)\n",
    "\n",
    "    # Check for multiple classes\n",
    "    if len(y_pred[0]) > 1:\n",
    "        print(\"doing Multiple-class classification\")\n",
    "\n",
    "        # We have to reshape our predictions to get them ready for plotting\n",
    "        y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n",
    "    else:\n",
    "        print(\"doing binary classification\")\n",
    "        y_pred = np.round(y_pred).reshape(xx.shape)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())  \n",
    "    plt.show()  # Added to display the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the prediction our model is makeing\n",
    "plot_decision_boundary(model=model_2, X=X, y=y)\n",
    "\n",
    "# This model is trying to make predictions as a line, meanwhile our data is a circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see if our model can be used for a regression problem...\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create some regression data\n",
    "X_regression = tf.range(0, 1000, 5)\n",
    "y_regression = tf.range(100, 1100, 5) # Givin this data, becasue the relationship we trying to predict is y = X +100\n",
    "\n",
    "\n",
    "X_regression, y_regression\n",
    "\n",
    "# Split our regression data into training and test sets\n",
    "X_reg_train = X_regression[:150]\n",
    "X_regression_test = X_regression[150:]\n",
    "y_reg_train = y_regression[:150]\n",
    "y_reg_test = y_regression[150:]\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model_2.fit(X_reg_train, y_reg_train, epochs=100)\n",
    "\n",
    "# It won't work because we compiled the model for a binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a new model\n",
    "model_3 = tf.keras.Sequential([\n",
    "    tf.keras.Dense(100),\n",
    "    tf.keras.Dense(10),\n",
    "    tf.keras.Dense(1)\n",
    "])\n",
    "\n",
    "#2. Compile the model\n",
    "model_3.compile(loss= tf.keras.losses.mae,\n",
    "                optimizer = tf.train.keras.Adam(),\n",
    "                metrics = \"mae\")\n",
    "\n",
    "# Fit the model\n",
    "model_3.fit(X_reg_train, y_reg_train, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with our trained model\n",
    "y_reg_pred = model_3.predict(X_regression_test)\n",
    "\n",
    "#Plot the model predictions against our regression data\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_reg_train, y_reg_train, c=\"b\", label=\"Trainig data\")\n",
    "plt.scatter(X_regression_test, y_reg_test, c=\"g\", label=\"Test data\")\n",
    "plt.scatter(X_regression_test, y_reg_pred, c=\"r\", label=\"Predictions data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The missing piece Non Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a new model\n",
    "model_4 = tf.keras.Sequential([\n",
    "    tf.keras.Dense(1, activation=tf.keras.activations.linear)\n",
    "])\n",
    "\n",
    "#2. Compile the model\n",
    "model_4.compile(loss= \"binary_crossentropy\",\n",
    "                optimizer = tf.train.keras.Adam(lr=0.001),\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "history = model_4.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out our data\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the desidion boundry for our model 4\n",
    "plot_decision_boundary(model= model_4, X=X, y=y)\n",
    "\n",
    "#This model is still preddictin in linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build our first NN with non linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a new model\n",
    "model_5 = tf.keras.Sequential([\n",
    "    tf.keras.Dense(1, activation=tf.keras.activations.relu) # Changed the activation to relu\n",
    "])\n",
    "\n",
    "#2. Compile the model\n",
    "model_5.compile(loss= \"binary_crossentropy\",\n",
    "                optimizer = tf.train.keras.Adam(lr=0.001),\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "history = model_5.fit(X, y, epochs=100)\n",
    "\n",
    "## This is worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to make the right model this time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a new model\n",
    "model_6 = tf.keras.Sequential([\n",
    "    tf.keras.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.Dense(1),\n",
    "])\n",
    "\n",
    "#2. Compile the model\n",
    "model_6.compile(loss= \"binary_crossentropy\",\n",
    "                optimizer = tf.train.keras.Adam(lr=0.001),\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "history = model_6.fit(X, y, epochs=250)\n",
    "\n",
    "## This is super worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model predictions\n",
    "plot_decision_boundary(model=model_6, X=X , y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Tweeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a new model\n",
    "model_7 = tf.keras.Sequential([\n",
    "    tf.keras.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.Dense(4, activation=\"relu\"),\n",
    "    tf.keras.Dense(1, activation=\"sigmoid\"), # The output layer contains a single neuron in order to make predictions. \n",
    "    # It uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1 that can easily and automatically be converted to crisp class values.\n",
    "])\n",
    "\n",
    "#2. Compile the model\n",
    "model_7.compile(loss= \"binary_crossentropy\",\n",
    "                optimizer = tf.train.keras.Adam(lr=0.001),\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "history = model_7.fit(X, y, epochs=100)\n",
    "\n",
    "## This is the best model so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our model\n",
    "model_7.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the metrics\n",
    "plot_decision_boundary(model=model_7, X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question: Whats is wrong with the predictions we have made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Commonly used activation functions in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ReLU` (Rectified Linear Unit):\n",
    "\n",
    "    * Activation Function: \n",
    "        f(x)=max(0,x)\n",
    "    * Characteristics:\n",
    "    - ReLU is one of the most widely used activation functions in deep learning.\n",
    "    - It returns the input if it is positive, otherwise returns zero.\n",
    "    - Simple and computationally efficient.\n",
    "    - Helps alleviate the vanishing gradient problem.\n",
    "    - However, it may suffer from the \"dying ReLU\" problem where neurons output zero for all inputs (especially during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Sigmoid`:\n",
    "\n",
    "    * Characteristics:\n",
    "    - RSigmoid squashes the output between 0 and 1.\n",
    "    - IIt is commonly used in binary classification problems where the output represents    probabilities.\n",
    "    - However, it may suffer from the vanishing gradient problem, particularly during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Tanh ` (Hyperbolic Tangent):\n",
    "\n",
    "    * Characteristics:\n",
    "    - Tanh squashes the output between -1 and 1.\n",
    "    - It is symmetric around the origin.\n",
    "    - It helps alleviate the vanishing gradient problem better than sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Softmax `:\n",
    "\n",
    "    * Characteristics:\n",
    "    - Softmax squashes the output into a probability distribution over multiple classes.\n",
    "    - It is commonly used in multi-class classification problems.\n",
    "    - The outputs sum up to 1, making them interpretable as class probabilities.\n",
    "    - It amplifies the largest input and suppresses the smaller ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Leaky ReLU `:\n",
    "\n",
    "    * Characteristics:\n",
    "    - Leaky ReLU addresses the \"dying ReLU\" problem by allowing a small, non-zero gradient when the input is negative.\n",
    "    - It helps prevent the issue of neurons becoming inactive during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ELU `  (Exponential Linear Unit):\n",
    "\n",
    "    * Characteristics:\n",
    "    - ELU is similar to Leaky ReLU but with an exponential component for negative inputs.\n",
    "    - It can help improve learning speed and performance compared to other activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy tensor\n",
    "A = tf.cast(tf.range(-10, 10), tf.float32)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our toy tensor\n",
    "plt.plot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start by replicating sigmoid : sigmoid(x) = 1 / (1 + exp(-x))\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + tf.exp(-x))\n",
    "#  Use the sigmoid function on our toy tensor\n",
    "sigmoid(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our toy tensor transformation by sigmoid function\n",
    "plt.plot(sigmoid(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toying with relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return tf.maximum(0, x)\n",
    "\n",
    "# Pass our toy tensor to the function\n",
    "relu(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ReLu - modified tensor\n",
    "plt.plot(relu(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toying with linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.activations.linear(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot linear activation\n",
    "plt.plot(tf.keras.activations.linear(A))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
